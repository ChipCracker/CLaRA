[languages]
primary = "en-US"
secondary = ["de-DE"]

[llm]
# LM Studio (OpenAI-Style) – Example-Config
provider = "lm-studio"  # or "openai" / "lm-studio" / "ollama"
model = "nvidia/nemotron-3-nano"
api_url = "http://host.docker.internal:1234/v1" # Use host.docker.internal for Mac/Docker Desktop
timeout_seconds = 180

# Ollama
#[llm]
#provider = "ollama"
#model = "qwen3:4b"
#max_tokens = 256
#temperature = 0.2
#timeout_seconds = 600

[checks]
enable_codespell = false
severity_threshold = "warning"  # Hook exits ≥ warning

[fixer]
safety_ratio = 0.0              # Min similarity (0.0-1.0) for auto-fix; lower = more aggressive
max_length_delta_ratio = 1.0    # Max length change ratio (0.0-1.0); higher = allow bigger changes

[paths]
include = ["tex/**/*.tex", "tex/**/*.bib"]
exclude = ["build/**", "out/**", "vendor/**"]
